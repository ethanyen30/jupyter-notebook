{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1_JE6oFMmke"
   },
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|skip\n",
    "#! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ydg2KZZJMmkj"
   },
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dur_1ENiMmkk"
   },
   "outputs": [],
   "source": [
    "#|all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbXUUZz8Mmkl"
   },
   "source": [
    "# Transfer learning in text\n",
    "\n",
    "> How to fine-tune a language model and train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj2Z02RJMmkm"
   },
   "source": [
    "In this tutorial, we will see how we can train a model to classify text (here based on their sentiment). First we will see how to do this quickly in a few lines of code, then how to get state-of-the art results using the approach of the [ULMFit paper](https://arxiv.org/abs/1801.06146).\n",
    "\n",
    "We will use the IMDb dataset from the paper [Learning Word Vectors for Sentiment Analysis](https://ai.stanford.edu/~amaas/data/sentiment/), containing a few thousand movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wndMsrf0Mmkn"
   },
   "source": [
    "## Train a text classifier from a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe1Yx59zMmko"
   },
   "source": [
    "We will try to train a classifier using a pretrained model, a bit like we do in the [vision tutorial](http://docs.fast.ai/tutorial.vision). To get our data ready, we will first use the high-level API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6Dvv2r2Mmkq"
   },
   "source": [
    "## Using the high-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAaLS8QCMmkr"
   },
   "source": [
    "We can download the data and decompress it with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xjH634-JMmks",
    "outputId": "bbaa9218-ed26-4fb9-ad0a-f16e5b26e2c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#8) [Path('C:/Users/ethan/.fastai/data/imdb/imdb.vocab'),Path('C:/Users/ethan/.fastai/data/imdb/models'),Path('C:/Users/ethan/.fastai/data/imdb/README'),Path('C:/Users/ethan/.fastai/data/imdb/test'),Path('C:/Users/ethan/.fastai/data/imdb/tmp_clas'),Path('C:/Users/ethan/.fastai/data/imdb/tmp_lm'),Path('C:/Users/ethan/.fastai/data/imdb/train'),Path('C:/Users/ethan/.fastai/data/imdb/unsup')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y2FRZiUjMmku",
    "outputId": "058a0d1b-dba2-4819-e6a3-d276a017edf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [Path('C:/Users/ethan/.fastai/data/imdb/train/labeledBow.feat'),Path('C:/Users/ethan/.fastai/data/imdb/train/neg'),Path('C:/Users/ethan/.fastai/data/imdb/train/pos'),Path('C:/Users/ethan/.fastai/data/imdb/train/unsupBow.feat')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jdIqmSYMmku"
   },
   "source": [
    "The data follows an ImageNet-style organization, in the train folder, we have two subfolders, `pos` and `neg` (for positive reviews and negative reviews). We can gather it by using the `TextDataLoaders.from_folder` method. The only thing we need to specify is the name of the validation folder, which is \"test\" (and not the default \"valid\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6kSn-5cpMmkv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to IPython and Windows limitation, python multiprocessing isn't available now.\n",
      "So `number_workers` is changed to 0 to avoid getting stuck\n"
     ]
    }
   ],
   "source": [
    "dls = TextDataLoaders.from_folder(path, valid='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.text.learner.TextLearner"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnghz3duMmkw"
   },
   "source": [
    "We can then have a look at the data with the `show_batch` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Sz-xglKzMmkw",
    "outputId": "a6b5431f-6387-4c6a-e453-8b10288e1283"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n",
      "C:\\Users\\ethan\\Downloads\\anaconda\\lib\\site-packages\\fastai\\torch_core.py:500: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ax = ax.append(pd.Series({label: o}))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj berlin - born in 1942 xxmaj margarethe von xxmaj trotta was an actress and now she is a very important director and writer . xxmaj she has been described , perhaps even unfairly caricatured , as a director whose commitment to bringing a woman 's sensibility to the screen outweighs her artistic strengths . \" rosenstrasse , \" which has garnered mixed and even strange reviews ( the xxmaj new xxmaj york xxmaj times article was one of the most negatively aggressive reviews xxmaj i 've ever read in that paper ) is not a perfect film . xxmaj it is a fine movie and a testament to a rare xxunk of successful opposition to the genocidal xxmaj nazi regime by , of all peoples , generically powerless xxmaj germans demonstrating in a xxmaj berlin street . \\n\\n xxmaj co - writer von xxmaj trotta uses the actual</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj prior to this release , xxmaj neil labute had this to say about the 1973 original : \" it 's surprising how many people say it 's their favorite soundtrack . xxmaj i 'm like , come on ! xxmaj you may not like the new one , but if that 's your favorite soundtrack , i do n't know if i * want * you to like my film . \" \\n\\n xxmaj neil , a word . xxmaj you might want to sit down for this too ; as xxmaj lord xxmaj xxunk says , shocks are so much better absorbed with the knees bent . xxmaj see , xxmaj neil , the thing about the original , is that xxmaj paul xxmaj giovanni 's soundtrack is one of the most celebrated things about it . xxmaj the filmmakers themselves consider it a virtual musical .</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos xxmaj how strange the human mind is ; this center of activity wherein perceptions of reality are formed and stored , and in which one 's view of the world hinges on the finely tuned functioning of the brain , this most delicate and intricate processor of all things sensory . xxmaj and how much do we really know of it 's inner - workings , of it 's depth or capacity ? xxmaj what is it in the mind that allows us to discern between reality and a dream ? xxmaj or can we ? xxmaj perhaps our sense of reality is no more than an impression of what we actually see , like looking at a painting by xxmaj monet , in which the vanilla sky of his vision becomes our reality . xxmaj it 's a concept visited by filmmaker xxmaj cameron xxmaj crowe in his</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos xxup spoilers xxup herein \\n\\n xxmaj my xxmaj high xxmaj school did all they could to try and motivate us for exams . xxmaj but the most memorable method they used to get us into the right state of mind was a guest speaker , who was none other than xxmaj australian xxmaj kickboxing 's favorite son , xxmaj stan \" the xxmaj man \" xxmaj xxunk . xxmaj the first mistake they made was giving this guy a microphone , because he was screaming half the time despite us sitting no more than 3 or 4 feet away from him . xxmaj now , his speech was full of the usual \" if you fail to prepare , then prepare to fail \" stuff , but there were various instances where i got really worked up . xxmaj the guy stood there in front of us preaching how</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos xxmaj beat a path to this important documentary that looks like an attractive feature . xxmaj forbidden xxmaj xxunk ) is simply a better ( cinematic ) version of xxmaj norma xxmaj khouri 's book xxmaj forbidden xxmaj love , and xxup that was a best - seller . xxmaj an onion - peeling of literary fraud and of a pretty woman , xxmaj xxunk is the very best in xxunk reality xxup tv . \\n\\n xxmaj cleverly edited and colourful , xxmaj broinowski 's storytelling is xxunk by moving silhouettes of xxmaj norma xxmaj khouri meaningfully blowing smoke . i disagree ( with xxmaj variety ) that it 's overlong ; instead my one slight problem was with the episodic nature of its key players commenting on others ' just - recorded testimonials . xxmaj on a single watching your sense of narrative becomes mired … .. so</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos * xxmaj some spoilers * \\n\\n xxmaj this movie is sometimes subtitled \" life xxmaj everlasting . \" xxmaj that 's often taken as reference to the final scene , but more accurately describes how dead and buried this once - estimable series is after this sloppy and illogical send - off . \\n\\n xxmaj there 's a \" hey kids , let 's put on a show air \" about this telemovie , which can be endearing in spots . xxmaj some fans will feel like insiders as they enjoy picking out all the various cameo appearances . xxmaj co - writer , co - producer xxmaj tom xxmaj fontana and his pals pack the goings - on with friends and favorites from other shows , as well as real xxmaj baltimore personages . \\n\\n xxmaj that 's on top of the returns of virtually all the members</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos i saw this movie during a xxmaj tolkien - themed xxmaj interim class during my sophomore year of college . i was seated unfortunately close to the screen and my professor chose me to serve as a whipping boy- everyone else was laughing , but they were n't within constant eyesight . \\n\\n xxmaj let 's get it out of the way : the xxmaj peter xxmaj jackson ' lord of the xxmaj rings ' films do owe something to the xxmaj bakshi film . xxmaj in xxmaj jackson 's version of xxmaj the xxmaj fellowship of the xxmaj ring , for instance , the scene in which the xxmaj black xxmaj riders assault the empty inn beds is almost a complete carbon copy of the scene in xxmaj bakshi 's film , shot by shot . xxmaj you could call this plagiarism or homage , depending on your</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos \" the xxmaj blob \" qualifies as a cult sci - fi film not only because it launched 27 - year old xxmaj steve mcqueen on a trajectory to superstardom , but also because it exploited the popular themes both of alien invasion and teenage delinquency that were inseparable in the 1950s . xxmaj interestingly , nobody in the xxmaj kay xxmaj xxunk &amp; xxmaj theodore xxmaj simonson screenplay ever refers to the amorphous , scarlet - red protoplasm that plummeted to xxmaj earth in a meteor and menaced everybody in the small town of xxmaj xxunk xxmaj pennsylvania on a xxmaj friday night as \" the xxmaj blob . \" xxmaj steve mcqueen won the role of xxmaj josh xxmaj randall , the old xxmaj west bounty hunter in \" wanted : xxmaj dead or xxmaj alive , \" after producer xxmaj dick xxmaj powell saw this xxmaj</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpZiTGkbMmkw"
   },
   "source": [
    "We can see that the library automatically processed all the texts to split then in *tokens*, adding some special tokens like:\n",
    "\n",
    "- `xxbos` to indicate the beginning of a text\n",
    "- `xxmaj` to indicate the next word was capitalized\n",
    "\n",
    "Then, we can define a `Learner` suitable for text classification in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "44JHcBYFMmkx"
   },
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mon9sA_DMmkx"
   },
   "source": [
    "We use the [AWD LSTM](https://arxiv.org/abs/1708.02182) architecture, `drop_mult` is a parameter that controls the magnitude of all dropouts in that model, and we use `accuracy` to track down how well we are doing. We can then fine-tune our pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDqdQGjQMmky",
    "outputId": "bae13c9d-bb46-4f6b-fe02-8da20e4a7356"
   },
   "outputs": [],
   "source": [
    "learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEol122JMmky",
    "outputId": "482c51ed-74c4-4a26-f78e-814ccc81d419"
   },
   "outputs": [],
   "source": [
    "#learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLXgmPVmMmkz"
   },
   "source": [
    "Not too bad! To see how well our model is doing, we can use the `show_results` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MZr0oYKMmkz",
    "outputId": "a78f8dab-2709-45d9-a483-91789e0d37ed"
   },
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6Slaw-lMmkz"
   },
   "source": [
    "And we can predict on new texts quite easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HUPhdI-Mmk0",
    "outputId": "dfe3ca33-c0dc-4e98-da67-bb2fd564c25f"
   },
   "outputs": [],
   "source": [
    "learn.predict(\"That honestly was the worst movie I've ever seen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZg5oCpmMmk0"
   },
   "source": [
    "Here we can see the model has considered the review to be positive. The second part of the result is the index of \"pos\" in our data vocabulary and the last part is the probabilities attributed to each class (99.1% for \"pos\" and 0.9% for \"neg\"). \n",
    "\n",
    "Now it's your turn! Write your own mini movie review, or copy one from the Internet, and we can see what this model thinks about it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElX0YVPKMmk1"
   },
   "source": [
    "### Using the data block API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCHQ11wCMmk1"
   },
   "source": [
    "We can also use the data block API to get our data in a `DataLoaders`. This is a bit more advanced, so fell free to skip this part if you are not comfortable with learning new APIs just yet.\n",
    "\n",
    "A datablock is built by giving the fastai library a bunch of information:\n",
    "\n",
    "- the types used, through an argument called `blocks`: here we have images and categories, so we pass `TextBlock` and `CategoryBlock`. To inform the library our texts are files in a folder, we use the `from_folder` class method.\n",
    "- how to get the raw items, here our function `get_text_files`.\n",
    "- how to label those items, here with the parent folder.\n",
    "- how to split those items, here with the grandparent folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RotFyNqlMmk2"
   },
   "outputs": [],
   "source": [
    "imdb = DataBlock(blocks=(TextBlock.from_folder(path), CategoryBlock),\n",
    "                 get_items=get_text_files,\n",
    "                 get_y=parent_label,\n",
    "                 splitter=GrandparentSplitter(valid_name='test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtA3XDM6Mmk3"
   },
   "source": [
    "This only gives a blueprint on how to assemble the data. To actually create it, we need to use the `dataloaders` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWWHbeIHMmk3"
   },
   "outputs": [],
   "source": [
    "dls = imdb.dataloaders(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOkeJIPmMmk3"
   },
   "source": [
    "## The ULMFiT approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6TcaNVjMmk4"
   },
   "source": [
    "The pretrained model we used in the previous section is called a language model. It was pretrained on Wikipedia on the task of guessing the next word, after reading all the words before. We got great results by directly fine-tuning this language model to a movie review classifier, but with one extra step, we can do even better: the Wikipedia English is slightly different from the IMDb English. So instead of jumping directly to the classifier, we could fine-tune our pretrained language model to the IMDb corpus and *then* use that as the base for our classifier.\n",
    "\n",
    "One reason, of course, is that it is helpful to understand the foundations of the models that you are using. But there is another very practical reason, which is that you get even better results if you fine tune the (sequence-based) language model prior to fine tuning the classification model. For instance, in the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached in the unsup folder. We can use all of these reviews to fine tune the pretrained language model — this will result in a language model that is particularly good at predicting the next word of a movie review. In contrast, the pretrained model was trained only on Wikipedia articles.\n",
    "\n",
    "The whole process is summarized by this picture:\n",
    "\n",
    "![ULMFit process](https://github.com/fastai/fastai/blob/master/nbs/images/ulmfit.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjtYi9B3Mmk4"
   },
   "source": [
    "### Fine-tuning a language model on IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6tKHkr_Mmk4"
   },
   "source": [
    "We can get our texts in a `DataLoaders` suitable for language modeling very easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRWjlz-6Mmk5"
   },
   "outputs": [],
   "source": [
    "dls_lm = TextDataLoaders.from_folder(path, is_lm=True, valid_pct=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWP7F2L8Mmk5"
   },
   "source": [
    "We need to pass something for `valid_pct` otherwise this method will try to split the data by using the grandparent folder names. By passing `valid_pct=0.1`, we tell it to get a random 10% of those reviews for the validation set.\n",
    "\n",
    "We can have a look at our data using `show_batch`. Here the task is to guess the next word, so we can see the targets have all shifted one word to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pFHpCw7Mmk5",
    "outputId": "64e1c8bb-3acd-4465-ee99-f54bda992ad5"
   },
   "outputs": [],
   "source": [
    "dls_lm.show_batch(max_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vu2ADi6dMmk6"
   },
   "source": [
    "Then we have a convenience method to directly grab a `Learner` from it, using the `AWD_LSTM` architecture like before. We use accuracy and perplexity as metrics (the later is the exponential of the loss) and we set a default weight decay of 0.1. `to_fp16` puts the `Learner` in mixed precision, which is going to help speed up training on GPUs that have Tensor Cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HppOzad9Mmk6"
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, wd=0.1).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaNt9kbBMmk7"
   },
   "source": [
    "By default, a pretrained `Learner` is in a frozen state, meaning that only the head of the model will train while the body stays frozen. We show you what is behind the fine_tune method here and use a fit_one_cycle method to fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7zsMRGHMmk7",
    "outputId": "228d73ce-df87-41db-e9f1-e224e9b1f4ad"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNPVcUuFMmk7"
   },
   "source": [
    "This model takes a while to train, so it's a good opportunity to talk about saving intermediary results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKbDOCLtMmk8"
   },
   "source": [
    "You can easily save the state of your model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k41xPRdJMmk8"
   },
   "outputs": [],
   "source": [
    "learn.save('1epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2hRX4F5Mmk8"
   },
   "source": [
    "It will create a file in `learn.path/models/` named \"1epoch.pth\". If you want to load your model on another machine after creating your `Learner` the same way, or resume training later, you can load the content of this file with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UytHJYtlMmk8"
   },
   "outputs": [],
   "source": [
    "learn = learn.load('1epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj8M5rnJMmk9"
   },
   "source": [
    "We can them fine-tune the model after unfreezing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1ohHNzaMmk9",
    "outputId": "6023188b-7527-436f-8eef-36ef898ebebe"
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYPLwgLZMmk9"
   },
   "source": [
    "Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the *encoder*. We can save it with `save_encoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9chJi5xUMmk9"
   },
   "outputs": [],
   "source": [
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMe_oQBlMmk-"
   },
   "source": [
    "> Jargon: Encoder: The model not including the task-specific final layer(s). It means much the same thing as *body* when applied to vision CNNs, but tends to be more used for NLP and generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JIFMo7wMmk-"
   },
   "source": [
    "Before using this to fine-tune a classifier on the reviews, we can use our model to generate random reviews: since it's trained to guess what the next word of the sentence is, we can use it to write new reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjJIfQkmMmk-",
    "outputId": "9ffe8140-78c7-4f36-b870-026f2171f9ff"
   },
   "outputs": [],
   "source": [
    "TEXT = \"I liked this movie by Robert Downey Jr. because\"\n",
    "N_WORDS = 30\n",
    "N_SENTENCES = 1\n",
    "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n",
    "         for _ in range(N_SENTENCES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yo1DzhvVMmk_",
    "outputId": "4b7ce369-f46c-4ff7-da77-5af043a00d0f"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZN1m28FMmk_"
   },
   "source": [
    "### Training a text classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AY6--yS9Mmk_"
   },
   "source": [
    "We can gather our data for text classification almost exactly like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ql4oek-kMmk_"
   },
   "outputs": [],
   "source": [
    "dls_clas = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test', text_vocab=dls_lm.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eH26NmiMmlA"
   },
   "source": [
    "The main difference is that we have to use the exact same vocabulary as when we were fine-tuning our language model, or the weights learned won't make any sense. We pass that vocabulary with `text_vocab`.\n",
    "\n",
    "Then we can define our text classifier like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LQFHU17MmlA"
   },
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neaP_xseMmlA"
   },
   "source": [
    "The difference is that before training it, we load the previous encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzABdxH0MmlA"
   },
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HudFSLNMmlB"
   },
   "source": [
    "The last step is to train with discriminative learning rates and *gradual unfreezing*. In computer vision, we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45uoNIo7MmlB",
    "outputId": "f586067e-05f6-4afd-e996-524fa370d66a"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-uhe9amMmlB"
   },
   "source": [
    "In just one epoch we get the same result as our training in the first section, not too bad! We can pass `-2` to `freeze_to` to freeze all except the last two parameter groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqliwURqMmlB",
    "outputId": "ac150477-e730-438d-eb4c-aa7d99274f31"
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30UtLOaYMmlC"
   },
   "source": [
    "Then we can unfreeze a bit more, and continue training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aCKQcCnMmlC",
    "outputId": "ad829256-1fae-4f78-a7f9-e0a46394a744"
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3WlRicvMmlC"
   },
   "source": [
    "And finally, the whole model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "236u-u8HMmlD",
    "outputId": "1dd0edb1-5fc9-4ff8-8563-6a9f9c50dc87"
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVuNxjtTMmlD"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "38_tutorial.text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
